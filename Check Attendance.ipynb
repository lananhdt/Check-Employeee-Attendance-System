{
 "cells": [
  {
   "cell_type": "code",
   "id": "66cd8cd9-123e-4466-92f1-4a6346da3c15",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc2da20-b2e0-4aca-bcc2-0d7cf73b05ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1w7Riq_itzcijqkvSW0yyivvlw0IgRGa_\n",
      "To: /Users/86store/AIO2025/Module 2/Week 4/Dataset.zip\n",
      "100%|██████████████████████████████████████| 1.34M/1.34M [00:00<00:00, 3.65MB/s]\n",
      "Archive:  Dataset.zip\n",
      "   creating: /Users/86store/AIO2025/Module 2/Week 4/Dataset\n",
      "  inflating: Dataset/Avatar_Aaron_Eckhart.jpg  \n",
      "  inflating: Dataset/Avatar_Aaron_Guiel.jpg  \n",
      "  inflating: Dataset/Avatar_Amy_Pascal.jpg  \n",
      "  inflating: Dataset/Avatar_Amy_Redford.jpg  \n",
      "  inflating: Dataset/Avatar_Andrew_Bernard.jpg  \n",
      "  inflating: Dataset/Avatar_Andrew_Cuomo.jpg  \n",
      "  inflating: Dataset/Avatar_Anh_Khoi.JPG  \n",
      "  inflating: Dataset/Avatar_Anil_Ramsook.jpg  \n",
      "  inflating: Dataset/Avatar_Camille_Lewis.jpg  \n",
      "  inflating: Dataset/Avatar_Carla_Gay_Balingit.jpg  \n",
      "  inflating: Dataset/Avatar_Dang_Nha.jpg  \n",
      "  inflating: Dataset/Avatar_Hoang_Nguyen.jpg  \n",
      "  inflating: Dataset/Avatar_Minh_Chau.jpg  \n",
      "  inflating: Dataset/Avatar_Phuc_Thinh.JPG  \n",
      "  inflating: Dataset/Avatar_Quoc_Thai.JPG  \n",
      "  inflating: Dataset/Avatar_Thang_Duong.jpg  \n",
      "  inflating: Dataset/Avatar_Thuan_Duong.jpg  \n",
      "  inflating: Dataset/Avatar_Tien_Huy.jpg  \n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "!gdown 1w7Riq_itzcijqkvSW0yyivvlw0IgRGa_\n",
    "!unzip Dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d21ecf-3fd0-4316-acec-fbf1b42e30d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing\n",
    "dataset_path = 'Dataset'\n",
    "os.listdir(dataset_path)\n",
    "image_paths = []\n",
    "labels = []\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if filename.endswith(('.jpg', '.JPG', '.png', '.jpeg')):\n",
    "        image_paths.append(os.path.join(dataset_path, filename))\n",
    "        file_name = filename.split('.')[0]\n",
    "        label = file_name[7:]\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0465373c-c6df-4bee-ad08-7facd15c94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'image_path': image_paths, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da308e89-cf27-489f-9c14-b8a26b24d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize Image\n",
    "IMAGE_SIZE = 300\n",
    "VECTOR_DIM = 300 * 300 * 3 # Vector Dimension for RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "509d781d-e024-477c-90e5-566646097041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS Index\n",
    "index = faiss.IndexFlatL2(VECTOR_DIM)\n",
    "label_map = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04bd8e02-ea2d-45fd-b402-a020db06feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_vector(image_path):\n",
    "    \"\"\"Convert image to normalized vector\"\"\"\n",
    "    img = Image.open(image_path).resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    img_array = np.array(img)\n",
    "\n",
    "    # Handle grayscale images (convert to RGB)\n",
    "    if len(img_array.shape) == 2:\n",
    "        img_array = np.stack((img_array,)*3, axis=-1)\n",
    "\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    vector = img_array.astype('float32') / 255.0\n",
    "    return vector.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75e70530-8bb2-4a6e-abcc-5caa4311e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng index từ dataframe\n",
    "for idx, row in df.iterrows():\n",
    "    # Get image_path, label from dataframe\n",
    "    image_path = row['image_path']\n",
    "    label = row['label']\n",
    "\n",
    "    try:\n",
    "        # Convert each imape_path to vector by image_to_vector function\n",
    "        vector = image_to_vector(image_path)\n",
    "\n",
    "        # Add to Faiss \n",
    "        index.add(np.array([vector]))\n",
    "        label_map.append(label)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a80876f-792b-4a34-9c98-e3e3f9b3a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the index and label map for later use\n",
    "faiss.write_index(index, \"employee_images.index\")\n",
    "np.save(\"label_map.npy\", np.array(label_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1d625c1-1b2b-498f-b37c-4fc531055613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_images(query_image_path, k=5):\n",
    "    \"\"\"Search for similar employee images\"\"\"\n",
    "    # Load index and labels\n",
    "    index = faiss.read_index(\"employee_images.index\")\n",
    "    label_map = np.load(\"label_map.npy\")\n",
    "\n",
    "    # Convert query image to vector\n",
    "    query_vector = image_to_vector(query_image_path)\n",
    "\n",
    "    # Search in Faiss\n",
    "    distances, indices = index.search(np.array([query_vector]), k)\n",
    "\n",
    "    # Get result\n",
    "    result = []\n",
    "    for i in range(len(indices[0])):\n",
    "        employee_name = label_map[indices[0][i]]\n",
    "        distance = distances[0][i]\n",
    "        result.append((employee_name, distance))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fc91d20-110c-4cdb-894a-238545e22042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_query_and_top_matches(query_image_path):\n",
    "    query_img = Image.open(query_image_path)\n",
    "    query_img = query_img.resize((300, 300))\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(query_img)\n",
    "    plt.title(\"Query Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    matches = search_similar_images(query_image_path)\n",
    "\n",
    "    \"\"\"\n",
    "    Display the top 5 matching employee images with distances\n",
    "    \"\"\"\n",
    "    # Get the image paths for the result\n",
    "    top_matches = []\n",
    "    for name, distance in matches:\n",
    "        # Find the image path for this employee in df\n",
    "        img_path = df[df['label'] == name]['image_path'].values[0]\n",
    "        top_matches.append((name, distance, img_path))\n",
    "    top_matches\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i, (name, distance, img_path) in enumerate(top_matches):\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((300, 300))\n",
    "\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        plt.imshow()\n",
    "        plt.title(f\"{name}\\nDist: {distance:.2f}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d09058b9-7824-45f2-8fb2-050a53d03109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize ResNet50 model for feature extraction\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = models.resnet50(weights=weights)\n",
    "\n",
    "# Remove the last classification layer\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23892f51-dc6f-46e0-8c20-32f669b26986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pretrained model (for face recognition task)\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "face_recognition_model = InceptionResnetV1(pretrained='vggface2').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "257cf173-5ed1-4525-abd5-a8ea77eec186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)), # Resize to the model's input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1027b1f2-084c-4cc3-80d7-c89f90a9f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(image_path, model):\n",
    "    \"\"\"Extract features from an image using a given model\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = model(img_tensor)\n",
    "    return features.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a556b0c9-51f9-4ea2-8524-d60c73adfa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 18/18 [00:03<00:00,  4.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS index\n",
    "VECTOR_DIM = 512\n",
    "index = faiss.IndexFlatIP(VECTOR_DIM)\n",
    "label_map = []\n",
    "\n",
    "# Add feature vector to index\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    features = extract_feature(row['image_path'], face_recognition_model)\n",
    "    index.add(np.array([features]))\n",
    "    label_map.append(row['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "757e8c51-7fd6-402f-889b-98e36d90c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save index and labels\n",
    "faiss.write_index(index, \"facenet_features.index\")\n",
    "np.save(\"facenet_label_map.npy\", np.array(label_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26c58db1-4381-4f9b-b1d9-44076bcc415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_feature(image_path, model):\n",
    "    \"\"\"Convert image to face embedding using a pre-trained model\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0) # Add batch dimension\n",
    "\n",
    "    # Get the embedding\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        embedding = model(img_tensor)\n",
    "\n",
    "    # Return the embedding as a numpy array\n",
    "    return embedding.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4268a704-2cea-4cb3-b6ce-2dfc9674fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_images(query_image_path, k=5):\n",
    "    \"\"\"Search for similar employee images using VGG16 features\"\"\"\n",
    "    # Load index and labels\n",
    "    index = faiss.read_index(\"facenet_features.index\")\n",
    "    label_map = np.load(\"facenet_label_map.npy\")\n",
    "\n",
    "    # Convert query image to vector\n",
    "    query_vector = image_to_feature(query_image_path, face_recognition_model)\n",
    "\n",
    "    # Search in Faiss\n",
    "    similarities, indices = index.search(np.array([query_vector]), k)\n",
    "\n",
    "    # Get results\n",
    "    results = []\n",
    "    for i in range(len(indices[0])):\n",
    "        employee_name = label_map[indices[0][i]]\n",
    "        similarity = similarities[0][i]\n",
    "        results.append((employee_name, similarity))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64863b99-ffaa-4860-996e-ff3b09db45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_query_and_top_matches(query_image_path):\n",
    "    # Display query image\n",
    "    query_img = Image.open(query_image_path)\n",
    "    query_img = query_img.resize((300, 300))\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(query_img)\n",
    "    plt.title(\"Query Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Get matches\n",
    "    matches = search_similar_images(query_image_path)\n",
    "\n",
    "    # Display top matches\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, (name, similarity) in enumerate(matches):\n",
    "        # Find the image path for this employee\n",
    "        img_path = df[df['label'] == name]['image_path'].values[0]\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((300, 300))\n",
    "\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{name}\\nSimilarity: {similarity:.2f}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
